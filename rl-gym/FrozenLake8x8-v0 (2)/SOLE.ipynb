{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define State Action Space\n",
    "state_space_n = env.observation_space.n\n",
    "action_space_n = env.action_space.n\n",
    "state_action_space_n = state_space_n * action_space_n\n",
    "# Map State-Action\n",
    "def msa(state, action):\n",
    "    return state * action_space_n + action\n",
    "# Build Reward & Transition Matrix\n",
    "R = np.zeros([state_action_space_n])\n",
    "P = np.zeros([state_action_space_n, state_space_n])\n",
    "for state in range(state_space_n):\n",
    "    for action in range(action_space_n):\n",
    "        for p, s, r, _ in env.P[state][action]:\n",
    "            R[msa(state, action)] += p * r\n",
    "            P[msa(state, action), s] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 627 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate Optimal Q Values\n",
    "Q = np.zeros([state_action_space_n])\n",
    "PI = np.zeros([state_space_n, state_action_space_n])\n",
    "I = np.identity(state_action_space_n)\n",
    "for i in range(100):\n",
    "    # print(i)\n",
    "    # Build Policy Matrix\n",
    "    old_PI = PI.copy()\n",
    "    PI.fill(0)\n",
    "    for state in range(state_space_n):\n",
    "        PI[state, msa(state,0) + np.argmax(Q[msa(state,0):msa(state,action_space_n)])] = 1\n",
    "    # print('PI', np.array_equal(old_PI, PI), state_space_n - sum(np.sum(old_PI == PI, axis=1) == state_action_space_n))\n",
    "    # Calculate Q Values\n",
    "    old_Q = Q\n",
    "    Q = np.linalg.solve(I - gamma * P @ PI, R)\n",
    "    # Q = np.linalg.inv(I - gamma * P @ PI) @ R\n",
    "    # print('Q ', np.array_equal(Q, old_Q), state_action_space_n - sum(Q == old_Q))\n",
    "    if np.array_equal(Q, old_Q): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.2986\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for _ in range(5000):\n",
    "    state = env.reset()\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        action = np.argmax(Q[msa(state,0):msa(state,action_space_n)])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        # clear_output(wait=True)\n",
    "        # env.render()\n",
    "        # print(i)\n",
    "        # time.sleep(0.01)\n",
    "        if done:\n",
    "            if not reward: i = 200\n",
    "            break\n",
    "    total += i\n",
    "print(total / 5000)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
