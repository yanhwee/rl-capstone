{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598887835641",
   "display_name": "Python 3.7.7 64-bit ('RL': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define State Action Space\n",
    "state_space_n = env.observation_space.n\n",
    "action_space_n = env.action_space.n\n",
    "state_action_space_n = state_space_n * action_space_n\n",
    "# Map State-Action\n",
    "def msa(state, action):\n",
    "    return state * action_space_n + action\n",
    "# Build Reward & Transition Matrix\n",
    "R = np.zeros([state_action_space_n])\n",
    "P = np.zeros([state_action_space_n, state_space_n])\n",
    "for state in range(state_space_n):\n",
    "    for action in range(action_space_n):\n",
    "        for p, s, r, _ in env.P[state][action]:\n",
    "            R[msa(state, action)] += p * r\n",
    "            P[msa(state, action), s] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 0 ns\n0\nPI False 500\nQ  False 3000\n1\nPI False 4\nQ  False 72\n2\nPI False 20\nQ  False 240\n3\nPI False 32\nQ  False 261\n4\nPI False 34\nQ  False 247\n5\nPI False 39\nQ  False 228\n6\nPI False 37\nQ  False 276\n7\nPI False 41\nQ  False 327\n8\nPI False 57\nQ  False 356\n9\nPI False 49\nQ  False 325\n10\nPI False 51\nQ  False 323\n11\nPI False 41\nQ  False 241\n12\nPI False 33\nQ  False 201\n13\nPI False 23\nQ  False 134\n14\nPI False 20\nQ  False 112\n15\nPI False 12\nQ  False 47\n16\nPI False 3\nQ  False 18\n17\nPI True 0\nQ  True 0\n"
    }
   ],
   "source": [
    "%time\n",
    "# Calculate Optimal Q Values\n",
    "Q = np.zeros([state_action_space_n])\n",
    "PI = np.zeros([state_space_n, state_action_space_n])\n",
    "I = np.identity(state_action_space_n)\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    # Build Policy Matrix\n",
    "    old_PI = PI.copy()\n",
    "    PI.fill(0)\n",
    "    for state in range(state_space_n):\n",
    "        PI[state, msa(state,0) + np.argmax(Q[msa(state,0):msa(state,action_space_n)])] = 1\n",
    "    print('PI', np.array_equal(old_PI, PI), state_space_n - sum(np.sum(old_PI == PI, axis=1) == state_action_space_n))\n",
    "    # Calculate Q Values\n",
    "    old_Q = Q\n",
    "    # Q = np.linalg.solve(I - gamma * P @ PI, R)\n",
    "    Q = np.linalg.inv(I - gamma * P @ PI) @ R\n",
    "    print('Q ', np.array_equal(Q, old_Q), state_action_space_n - sum(Q == old_Q))\n",
    "    if np.array_equal(Q, old_Q): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# while True:\n",
    "#     action = np.argmax(Q[msa(state,0):msa(state,action_space_n)])\n",
    "#     state, reward, done, info = env.step(action)\n",
    "\n",
    "#     clear_output(wait=True)\n",
    "#     env.render()\n",
    "#     time.sleep(0.25)\n",
    "#     if done: break\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Calculate Optimal V Values\n",
    "# V = np.zeros([state_space_n])\n",
    "# PI = np.zeros([state_space_n, state_action_space_n])\n",
    "# I = np.identity(state_space_n)\n",
    "# i = 0\n",
    "# while True:\n",
    "#     print(i)\n",
    "#     i += 1\n",
    "#     # Build Policy Matrix\n",
    "#     old_PI = PI.copy()\n",
    "#     PI.fill(0)\n",
    "#     for state in range(state_space_n):\n",
    "#         max_action = -1\n",
    "#         max_value = float('-inf')\n",
    "#         for action in range(action_space_n):\n",
    "#             value = sum([p * (r + V[s]) for p, s, r, _ in env.P[state][action]])\n",
    "#             if value > max_value:\n",
    "#                 max_action = action\n",
    "#                 max_value = value\n",
    "#         PI[state, msa(state, max_action)] = 1\n",
    "#     print('PI', np.array_equal(old_PI, PI), state_space_n - sum(np.sum(old_PI == PI, axis=1) == state_action_space_n))\n",
    "#     # Calculate V Values\n",
    "#     old_V = V\n",
    "#     V = np.linalg.solve(I - gamma * PI @ P - 1, PI @ R)\n",
    "#     # V = np.linalg.pinv(I - gamma * PI @ P) @ (PI @ R)\n",
    "#     print('V ', np.array_equal(V, old_V), state_space_n - sum(V == old_V))\n",
    "#     if np.array_equal(V, old_V): break"
   ]
  }
 ]
}